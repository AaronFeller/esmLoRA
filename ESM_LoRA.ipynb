{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version of esm\n",
    "%pip show fair-esm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import esm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import loralib as lora\n",
    "import esmLoRA.esm as el\n",
    "from esmLoRA.esm.model.esm2 import ESM2\n",
    "import pytorch_lightning as pl\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESM2(\n",
       "  (embed_tokens): Embedding(33, 320, padding_idx=1)\n",
       "  (layers): ModuleList(\n",
       "    (0): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (v_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (q_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "      (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      (final_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (v_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (q_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "      (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      (final_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (v_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (q_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "      (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      (final_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (v_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (q_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "      (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      (final_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (v_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (q_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "      (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      (final_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (v_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (q_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "      (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      (final_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (contact_head): ContactPredictionHead(\n",
       "    (regression): Linear(in_features=120, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load ESM model\n",
    "\n",
    "#model used on this exemple available on git, most recent version \n",
    "\n",
    "pretrained_model, alphabet = el.pretrained.esm2_t6_8M_UR50D()\n",
    "#model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "# disables dropout for deterministic results\n",
    "pretrained_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = 'esm2_t6_8M_UR50D_pretrained_checkpoint.pt'\n",
    "# torch.save(pretrained_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 20, 15, 11,  7, 10, 16,  9, 10,  4, 15,  8, 12,  7, 10, 12,  4,  9,\n",
       "         10,  8, 15,  9, 14,  7,  8,  6,  5, 16,  4,  5,  9,  9,  4,  8,  7,  8,\n",
       "         10, 16,  7, 12,  7, 16, 13, 12,  5, 19,  4, 10,  8,  4,  6, 19, 17, 12,\n",
       "          7,  5, 11, 14, 10,  6, 19,  7,  4,  5,  6,  6,  2]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(\"protein1\", \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\")]\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "batch_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logits': tensor([[[ 13.9791,  -9.0924,  -6.5645,  ..., -14.8934, -15.2000,  -9.0806],\n",
       "          [ -8.2908, -14.3483,  -9.2373,  ..., -15.6513, -15.9180, -14.3464],\n",
       "          [-13.0977, -23.2342, -13.0121,  ..., -16.0929, -16.0443, -23.2163],\n",
       "          ...,\n",
       "          [-11.8081, -23.6640, -13.1323,  ..., -16.5725, -16.6506, -23.6673],\n",
       "          [-11.6440, -21.9399, -11.6811,  ..., -16.2364, -16.2875, -21.9438],\n",
       "          [ -5.6318,  -6.4352,  19.2604,  ..., -16.4212, -16.2883,  -6.4788]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " 'representations': {}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_tokens[0]\n",
    "pretrained_model(batch_tokens, return_contacts=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ESM2(num_layers=6, embed_dim=320, attention_heads=20, token_dropout=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['layers.0.fc1.lora_A', 'layers.0.fc1.lora_B', 'layers.0.fc2.lora_A', 'layers.0.fc2.lora_B', 'layers.1.fc1.lora_A', 'layers.1.fc1.lora_B', 'layers.1.fc2.lora_A', 'layers.1.fc2.lora_B', 'layers.2.fc1.lora_A', 'layers.2.fc1.lora_B', 'layers.2.fc2.lora_A', 'layers.2.fc2.lora_B', 'layers.3.fc1.lora_A', 'layers.3.fc1.lora_B', 'layers.3.fc2.lora_A', 'layers.3.fc2.lora_B', 'layers.4.fc1.lora_A', 'layers.4.fc1.lora_B', 'layers.4.fc2.lora_A', 'layers.4.fc2.lora_B', 'layers.5.fc1.lora_A', 'layers.5.fc1.lora_B', 'layers.5.fc2.lora_A', 'layers.5.fc2.lora_B'], unexpected_keys=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = 'esm2_t6_8M_UR50D_pretrained_checkpoint.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(PATH), strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.load_state_dict(torch.load(PATH), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<cls>',\n",
       " 1: '<pad>',\n",
       " 2: '<eos>',\n",
       " 3: '<unk>',\n",
       " 4: 'L',\n",
       " 5: 'A',\n",
       " 6: 'G',\n",
       " 7: 'V',\n",
       " 8: 'S',\n",
       " 9: 'E',\n",
       " 10: 'R',\n",
       " 11: 'T',\n",
       " 12: 'I',\n",
       " 13: 'D',\n",
       " 14: 'P',\n",
       " 15: 'K',\n",
       " 16: 'Q',\n",
       " 17: 'N',\n",
       " 18: 'F',\n",
       " 19: 'Y',\n",
       " 20: 'M',\n",
       " 21: 'H',\n",
       " 22: 'W',\n",
       " 23: 'C',\n",
       " 24: 'X',\n",
       " 25: 'B',\n",
       " 26: 'U',\n",
       " 27: 'Z',\n",
       " 28: 'O',\n",
       " 29: '.',\n",
       " 30: '-',\n",
       " 31: '<null_1>',\n",
       " 32: '<mask>'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate mapping for tokens to amino acids\n",
    "token_mapping = alphabet.to_dict() # {amino_acid: token_id}\n",
    "token_mapping = {v: k for k, v in token_mapping.items()} # {token_id: amino_acid}\n",
    "token_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 20, 15, 11,  7, 10, 16,  9, 10,  4, 15,  8, 12,  7, 10, 12,  4,  9,\n",
       "         10,  8, 15,  9, 14,  7,  8,  6,  5, 16,  4,  5,  9,  9,  4,  8,  7,  8,\n",
       "         10, 16,  7, 12,  7, 16, 13, 12,  5, 19,  4, 10,  8,  4,  6, 19, 17, 12,\n",
       "          7,  5, 11, 14, 10,  6, 19,  7,  4,  5,  6,  6,  2,  1,  1,  1,  1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens = torch.Tensor([alphabet.encode(\"<cls>MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG<eos><pad><pad><pad><pad>\")]).to(torch.int64)\n",
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<cls>MSTVRQERLKSIVRILERHSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGGYVLAEG<eos>EEEE'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens = torch.Tensor([alphabet.encode(\"<cls>M<mask>TVRQERLKSIVRILER<mask>SKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRG<mask>YVLA<mask>G<eos><pad><pad><pad><pad>\")], )\n",
    "\n",
    "input_tokens.to(torch.int64)\n",
    "\n",
    "#pull logits from model\n",
    "var = pretrained_model.forward(input_tokens.int(), return_contacts = False)['logits']\n",
    "prediction = var[0,:].argmax(dim=1)\n",
    "\n",
    "reconstructed = ''\n",
    "for token in prediction:\n",
    "    reconstructed += token_mapping[token.item()]\n",
    "reconstructed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 20,  8, 11,  7, 10, 16,  9, 10,  4, 15,  8, 12,  7, 10, 12,  4,  9,\n",
       "        10, 21,  8, 15,  9, 14,  7,  8,  6,  5, 16,  4,  5,  9,  9,  4,  8,  7,\n",
       "         8, 10, 16,  7, 12,  7, 16, 13, 12,  5, 19,  4, 10,  8,  4,  6, 19, 17,\n",
       "        12,  7,  5, 11, 14, 10,  6,  6, 19,  7,  4,  5,  9,  6,  2,  9,  9,  9,\n",
       "         9])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var[0,:].argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3360133998.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [12], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    var = model.forward(input_tokens.int(), self_attn_mask = )['logits']\u001b[0m\n\u001b[0m                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#pull logits from model\n",
    "var = model.forward(input_tokens.int())['logits']\n",
    "prediction = var[0,:].argmax(dim=1)\n",
    "\n",
    "reconstructed = ''\n",
    "for token in prediction:\n",
    "#    print(token)\n",
    "    reconstructed += token_mapping[token.item()]\n",
    "reconstructed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora.mark_only_lora_as_trainable(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sp|A0A0B4J2F0|PIOS1_HUMAN</td>\n",
       "      <td>MFRRLTFAQLLFATVLGIAGGVYIFQPVFEQYAKDQKELKEKMQLV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sp|A0A0C5B5G6|MOTSC_HUMAN</td>\n",
       "      <td>MRWQEMGYIFYPRKLR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sp|A0A0K2S4Q6|CD3CH_HUMAN</td>\n",
       "      <td>MTQRAGAAMLPSALLLLCVPGCLTVSGPSTVMGAVGESLSVQCRYE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sp|A0A0U1RRE5|NBDY_HUMAN</td>\n",
       "      <td>MGDQPCASGRSTLPPGNAREAKPPKKRCLLAPRWDYPEGTPNGGST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sp|A0A1B0GTW7|CIROP_HUMAN</td>\n",
       "      <td>MLLLLLLLLLLPPLVLRVAASRCLHDETQKSVSLLRPPFSQLPSKS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12032</th>\n",
       "      <td>sp|Q9Y6X8|ZHX2_HUMAN</td>\n",
       "      <td>MASKRKSTTPCMVRTSQVVEQDVPEEVDRAKEKGIGTPQPDVAKDS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12033</th>\n",
       "      <td>sp|Q9Y6Y0|NS1BP_HUMAN</td>\n",
       "      <td>MIPNGYLMFEDENFIESSVAKLNALRKSGQFCDVRLQVCGHEMLAH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12034</th>\n",
       "      <td>sp|Q9Y6Y8|S23IP_HUMAN</td>\n",
       "      <td>MAERKPNGGSGGASTSSSGTNLLFSSSATEFSFNVPFIPVTQASAS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12035</th>\n",
       "      <td>sp|Q9Y6Y9|LY96_HUMAN</td>\n",
       "      <td>MLPFLFFSTLFSSIFTEAQKQYWVCNSSDASISYTYCDKMQYPISI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12036</th>\n",
       "      <td>sp|Q9Y6Z7|COL10_HUMAN</td>\n",
       "      <td>MNGFASLLRRNQFILLVLFLLQIQSLGLDIDSRPTAEVCATHTISP...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12037 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id  \\\n",
       "0      sp|A0A0B4J2F0|PIOS1_HUMAN   \n",
       "1      sp|A0A0C5B5G6|MOTSC_HUMAN   \n",
       "2      sp|A0A0K2S4Q6|CD3CH_HUMAN   \n",
       "3       sp|A0A0U1RRE5|NBDY_HUMAN   \n",
       "4      sp|A0A1B0GTW7|CIROP_HUMAN   \n",
       "...                          ...   \n",
       "12032       sp|Q9Y6X8|ZHX2_HUMAN   \n",
       "12033      sp|Q9Y6Y0|NS1BP_HUMAN   \n",
       "12034      sp|Q9Y6Y8|S23IP_HUMAN   \n",
       "12035       sp|Q9Y6Y9|LY96_HUMAN   \n",
       "12036      sp|Q9Y6Z7|COL10_HUMAN   \n",
       "\n",
       "                                                sequence  \n",
       "0      MFRRLTFAQLLFATVLGIAGGVYIFQPVFEQYAKDQKELKEKMQLV...  \n",
       "1                                       MRWQEMGYIFYPRKLR  \n",
       "2      MTQRAGAAMLPSALLLLCVPGCLTVSGPSTVMGAVGESLSVQCRYE...  \n",
       "3      MGDQPCASGRSTLPPGNAREAKPPKKRCLLAPRWDYPEGTPNGGST...  \n",
       "4      MLLLLLLLLLLPPLVLRVAASRCLHDETQKSVSLLRPPFSQLPSKS...  \n",
       "...                                                  ...  \n",
       "12032  MASKRKSTTPCMVRTSQVVEQDVPEEVDRAKEKGIGTPQPDVAKDS...  \n",
       "12033  MIPNGYLMFEDENFIESSVAKLNALRKSGQFCDVRLQVCGHEMLAH...  \n",
       "12034  MAERKPNGGSGGASTSSSGTNLLFSSSATEFSFNVPFIPVTQASAS...  \n",
       "12035  MLPFLFFSTLFSSIFTEAQKQYWVCNSSDASISYTYCDKMQYPISI...  \n",
       "12036  MNGFASLLRRNQFILLVLFLLQIQSLGLDIDSRPTAEVCATHTISP...  \n",
       "\n",
       "[12037 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "#read fasta file to csv with seqio\n",
    "fasta_file = 'data/uniprotkb_reviewed_true_AND_annotation_2023_07_12.fasta'\n",
    "\n",
    "with open(fasta_file) as fasta_file:  # Will close handle cleanly\n",
    "    id = []\n",
    "    sequence = []\n",
    "    for seq_record in SeqIO.parse(fasta_file, 'fasta'):  # (generator)\n",
    "        id.append(seq_record.id)\n",
    "        sequence.append(str(seq_record.seq))\n",
    "\n",
    "tuples = list(zip(id, sequence))\n",
    "\n",
    "#replace 15% of amino acids with mask token\n",
    "\n",
    "masked_sequence = []\n",
    "\n",
    "\n",
    "masked_tuples = list(zip(id, masked_sequence))\n",
    "\n",
    "# remove length > 1024\n",
    "tuples = [t for t in tuples if len(t[1]) <= 1022]\n",
    "\n",
    "#append cls and eos tokens to each sequence (NOT NEEDED)\n",
    "#tuples = [(t[0], \"<cls>\" + t[1] + \"<eos>\") for t in tuples]\n",
    "\n",
    "\n",
    "# df = pd.DataFrame(tuples, columns=['id', 'sequence'])\n",
    "# df = df[df['sequence'].str.len() <= 1022]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tuples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/stor/work/Wilke/afeller/esm_embeddings/ESM_LoRA.ipynb Cell 17\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bwilkcomp01/stor/work/Wilke/afeller/esm_embeddings/ESM_LoRA.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m data \u001b[39m=\u001b[39m tuples\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bwilkcomp01/stor/work/Wilke/afeller/esm_embeddings/ESM_LoRA.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m batch_labels, batch_strs, batch_tokens \u001b[39m=\u001b[39m batch_converter(data)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bwilkcomp01/stor/work/Wilke/afeller/esm_embeddings/ESM_LoRA.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m batch_lens \u001b[39m=\u001b[39m (batch_tokens \u001b[39m!=\u001b[39m alphabet\u001b[39m.\u001b[39mpadding_idx)\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tuples' is not defined"
     ]
    }
   ],
   "source": [
    "data = tuples\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "len(batch_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 974  536  738  833  116   71  739  710  280  608  847   51  385   25\n",
      "  506  327  223  238  222  304  574  248  757  923  618  194   56  905\n",
      "  662  831  529  488  172  136  183  512  471  496  398  699  215  515\n",
      "  992  783  924  474  343  854  598  601  296  243  639 1022  589  914\n",
      "  882  388  953  641  673  995  351  793  570  300  233  510  319  846\n",
      "  184  855  267  467  100   49  330  829 1011  767  278   29  239  414\n",
      "  554  256  700  192  719  723  594  202  617  160  159  632  271  776\n",
      "  227  759  431  152  216  975  607  844  899  817  746  727  796  921\n",
      "  910  242  753  380  264  788  721  112  720  148  983  778  542  786\n",
      "  576  858  421  221  708  447  365   22   90  821  763  915  125  465\n",
      "   63  665  893    9  954  903  354  866 1004  666  568  324  643]\n",
      "[ 198  971  574  571  371  132  370  771  780  702  273  947  156  262\n",
      "  514  785  324  580  705 1002   22  584  497  340  833  166  759  666\n",
      "   79  809  922  546  788   67  641  627  277  456 1013  883  435  384\n",
      "  648  683  349  935  977  147  895  207  784  212   44  813  108   41\n",
      "  322  424  800   17  214  615  775 1008  353   64  970  945  732  462\n",
      "  565  189  251  736  688 1021  185  698  105  864  915  229  369  828\n",
      "  777  106  164  893  928  524  396  940  245  604    7  154  287  267\n",
      "  778  638  431  253  496  211  203  690  910  411  178  776  525  750\n",
      "  908   38  143  246  819  210  290  416  755   26  237  146  489  345\n",
      "  631  508  653  134 1019  438  888  982  500  366  892  551  506  865\n",
      "  658  148  804  749  503  960  582  455  985  886  719  388  304]\n",
      "[ 292  742  441  633  661   30  406  994 1013   40  291   65  617  657\n",
      "  689  889  466  879  799  992  212  494  189  242  910  101   17   21\n",
      " 1011  485  571  165   89  804  807  953  836  872  449  227  210  792\n",
      "   82  249   24  164  495  995  132  694  296  458  300  432  745  285\n",
      "  645  465  310  874  587  160  327   34  469  204  134  586  731  579\n",
      "  238  222  203  127  456  347  536  748  492  124  304  103  723   58\n",
      "  843  376  435  487  342  372   26  721  943  179  813  951  184  519\n",
      "   44  113  181  107  667  853 1007  898  283  779   16  669  858  784\n",
      "  638  619  183  175  237  279  299  272  777  743  717 1006  578   28\n",
      "  979  670  467   76  252  188  681  947  615  411  593  700    5  684\n",
      "  676  572  361  931   36  767  511  861  774  789  757  876  282]\n"
     ]
    }
   ],
   "source": [
    "#Choose 0.15 of tokens to mask\n",
    "for i in range(0, len(batch_tokens[0:3])):\n",
    "    index_to_change = (np.random.choice(len(batch_tokens[i]), size=int(0.15*len(batch_tokens[i])), replace=False))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = torch.ones_like(batch_tokens)\n",
    "#make sure attention mask is 0 for padding tokens \"1\"\n",
    "attention_mask = attention_mask.masked_fill(batch_tokens == alphabet.padding_idx, 0)\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 20, 18,  ...,  1,  1,  1],\n",
       "        [ 0, 20, 10,  ...,  1,  1,  1],\n",
       "        [ 0, 20, 11,  ...,  1,  1,  1]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_tokens[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 20, 18,  ...,  4, 12,  6],\n",
       "        [ 0, 20, 10,  ...,  4,  4,  4],\n",
       "        [ 0, 20, 11,  ...,  4,  4,  8]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(batch_tokens[0:3])['logits'].argmax(dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESM2(\n",
       "  (embed_tokens): Embedding(33, 320, padding_idx=1)\n",
       "  (layers): ModuleList(\n",
       "    (0): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (v_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (q_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "      (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      (final_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (v_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (q_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "      (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      (final_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (v_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (q_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "      (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      (final_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (v_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (q_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "      (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      (final_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (v_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (q_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "      (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      (final_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (v_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (q_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "      (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      (final_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (contact_head): ContactPredictionHead(\n",
       "    (regression): Linear(in_features=120, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# and move our model over to the selected device\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name                 | Type                  | Params\n",
      "---------------------------------------------------------------\n",
      "0 | embed_tokens         | Embedding             | 10.6 K\n",
      "1 | layers               | ModuleList            | 7.7 M \n",
      "2 | contact_head         | ContactPredictionHead | 121   \n",
      "3 | emb_layer_norm_after | LayerNorm             | 640   \n",
      "4 | lm_head              | RobertaLMHead         | 113 K \n",
      "---------------------------------------------------------------\n",
      "2.9 M     Trainable params\n",
      "4.9 M     Non-trainable params\n",
      "7.8 M     Total params\n",
      "31.279    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e193e8a7bba47b18b1232296620de92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/stor/home/alf3564/miniconda3/envs/esm/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 112 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/stor/work/Wilke/afeller/esm_embeddings/ESM_LoRA.ipynb Cell 23\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwilkcomp01/stor/work/Wilke/afeller/esm_embeddings/ESM_LoRA.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# training\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwilkcomp01/stor/work/Wilke/afeller/esm_embeddings/ESM_LoRA.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(precision\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbf16-mixed\u001b[39m\u001b[39m'\u001b[39m, limit_train_batches\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, max_epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bwilkcomp01/stor/work/Wilke/afeller/esm_embeddings/ESM_LoRA.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, train_loader, val_loader)\n",
      "File \u001b[0;32m~/miniconda3/envs/esm/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:529\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    527\u001b[0m model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    528\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 529\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    530\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    531\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/esm/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:42\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     44\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     45\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/esm/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:568\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m    559\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m    560\u001b[0m )\n\u001b[1;32m    562\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    563\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    564\u001b[0m     ckpt_path,\n\u001b[1;32m    565\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    566\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    567\u001b[0m )\n\u001b[0;32m--> 568\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    570\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    571\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/esm/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:973\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    970\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 973\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    975\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    977\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/esm/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1014\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m   1013\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1014\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1015\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1016\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/miniconda3/envs/esm/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1043\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1040\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1042\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1045\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1047\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/esm/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py:177\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    176\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/esm/lib/python3.9/site-packages/pytorch_lightning/loops/evaluation_loop.py:115\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m     previous_dataloader_idx \u001b[39m=\u001b[39m dataloader_idx\n\u001b[1;32m    114\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx)\n\u001b[1;32m    116\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/esm/lib/python3.9/site-packages/pytorch_lightning/loops/evaluation_loop.py:375\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    374\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 375\u001b[0m output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, hook_name, \u001b[39m*\u001b[39;49mstep_kwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    379\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mon_test_batch_end\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mon_validation_batch_end\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/esm/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:291\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 291\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    293\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    294\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/esm/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py:379\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[1;32m    378\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, ValidationStep)\n\u001b[0;32m--> 379\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/stor/work/Wilke/afeller/esm_embeddings/esmLoRA/esm/model/esm2.py:167\u001b[0m, in \u001b[0;36mESM2.validation_step\u001b[0;34m(self, val_batch, batch_idx)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_step\u001b[39m(\u001b[39mself\u001b[39m, val_batch, batch_idx):\n\u001b[0;32m--> 167\u001b[0m     x, y \u001b[39m=\u001b[39m val_batch\n\u001b[1;32m    168\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(x)\n\u001b[1;32m    169\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcross_entropy_loss(logits, y)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from esmLoRA.esm.model.esm2 import ESM2\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# data\n",
    "dataset = batch_tokens\n",
    "train, val = random_split(dataset, [math.ceil(0.9*len(batch_tokens)), math.floor(0.1*len(batch_tokens))])\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=32)\n",
    "val_loader = DataLoader(val, batch_size=32)\n",
    "\n",
    "# model\n",
    "model = ESM2(num_layers=6, embed_dim=320, attention_heads=20, token_dropout=False)\n",
    "\n",
    "# training\n",
    "trainer = pl.Trainer(precision='bf16-mixed', limit_train_batches=0.5, max_epochs=10)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "\n",
    "  def setup(self, stage):\n",
    "    # transforms for images\n",
    "    transform=transforms.Compose([transforms.ToTensor(), \n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))])\n",
    "      \n",
    "    # prepare transforms standard to MNIST\n",
    "    self.mnist_train = MNIST(os.getcwd(), train=True, download=True, transform=transform)\n",
    "    self.mnist_test = MNIST(os.getcwd(), train=False, download=True, transform=transform)\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    return DataLoader(self.mnist_train, batch_size=64)\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    return DataLoader(self.mnist_test, batch_size=64)\n",
    "\n",
    "data_module = MNISTDataModule()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/stor/work/Wilke/afeller/esm_embeddings/ESM_LoRA.ipynb Cell 25\u001b[0m in \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bwilkcomp01/stor/work/Wilke/afeller/esm_embeddings/ESM_LoRA.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# train\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bwilkcomp01/stor/work/Wilke/afeller/esm_embeddings/ESM_LoRA.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m pretrained_model()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bwilkcomp01/stor/work/Wilke/afeller/esm_embeddings/ESM_LoRA.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bwilkcomp01/stor/work/Wilke/afeller/esm_embeddings/ESM_LoRA.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m trainer\u001b[39m.\u001b[39mfit(model, data_module)\n",
      "File \u001b[0;32m~/miniconda3/envs/esm/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'tokens'"
     ]
    }
   ],
   "source": [
    "# train\n",
    "model = pretrained_model()\n",
    "trainer = pl.Trainer()\n",
    "\n",
    "trainer.fit(model, data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
