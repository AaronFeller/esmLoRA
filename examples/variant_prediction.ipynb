{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/facebookresearch/esm/blob/master/examples/variant_prediction.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variant prediction with ESM\n",
    "\n",
    "This tutorial demonstrates how to train a variant predictor using representations from ESM. You can adopt a similar protocol to train a model for any downstream task, even with limited data.\n",
    "\n",
    "In this tutorial, we will build a simple downstream head in sklearn to predict the effects of mutations from ESM representations. All representations can be dumped before fitting the top model. Therefore, representations for your dataset can be dumped once using a GPU. Then, the rest of your analysis can be performed on CPU. \n",
    "\n",
    "### Background\n",
    "\n",
    "In this particular example, we will train a model to predict the activity of ß-lactamase variants.\n",
    "\n",
    "Training data is located in `examples/P62593.fasta`, a FASTA file where each entry contains:\n",
    "- the mutated ß-lactamase sequence, where a single residue is mutated (swapped with another amino acid)\n",
    "- a float describing the scaled effect of the mutation\n",
    "\n",
    "The data was retrieved from the Envision paper (Gray, et al. 2018).\n",
    "\n",
    "### Goals\n",
    "- Obtain a representation for each mutated sequence.\n",
    "- Train a regression model in sklearn that can predict the \"effect\" score given the representation.\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "- You will need the following modules : tqdm, matplotlib, numpy, pandas, seaborn, scipy, scikit-learn\n",
    "- You have obtained sequence representations for ß-lactamase either by:\n",
    "    - downloading them from [here](https://dl.fbaipublicfiles.com/fair-esm/example/P62593_reprs.tar.gz)\n",
    "    - running `python extract.py esm1_t34_670M_UR50S examples/P62593.fasta my_reprs/ --repr_layers 34 --include mean`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "1. [Prelims](#prelims)\n",
    "2. [Loading Model](#load_model)\n",
    "3. [Loading Representations](#load_representations)\n",
    "4. [Visualizing Representations](#viz_representations)\n",
    "5. [Initializing / Running Grid Search](#grid_search)\n",
    "6. [Browse Grid Search Results](#browse)\n",
    "7. [Evaluating Results](#eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prelims'></a>\n",
    "## Prelims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume you pip installed the repo as per the README. Otherwise, you can `sys.path.append(<path_to_repo>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# PATH_TO_REPO = \"\"\n",
    "# sys.path.append(PATH_TO_REPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the path to your representations here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPR_PATH = \"../my_reprs\" # Path to directory of representations for P62593.fasta\n",
    "FASTA_PATH = \"../examples/P62593.fasta\" # Path to P62593.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load_model'></a>\n",
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, alphabet = esm.pretrained.esm1_t34_670M_UR50S()\n",
    "batch_converter = alphabet.get_batch_converter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load_representations'></a>\n",
    "## Load Representations (Xs) and Target Effects (ys)\n",
    "Our FASTA file is formatted as such:\n",
    "```\n",
    ">{index}|{mutation_id}|{effect}\n",
    "{seq}\n",
    "```\n",
    "We will be extracting the effect from each entry.\n",
    "\n",
    "Our representations are stored under the file name `{index}|{mutation_id}|{effect}.pt`\n",
    "\n",
    "So, entries and representations should be linked by `{index}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = []\n",
    "for header, _seq in esm.data.read_fasta(FASTA_PATH):\n",
    "    scaled_effect = header.split('|')[-1]\n",
    "    ys.append(float(scaled_effect))\n",
    "print(len(ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = Path(REPR_PATH) / '*.pt'\n",
    "files = glob.glob(str(pattern))\n",
    "num_datapoints = len(files)\n",
    "dimension = 1280\n",
    "Xs = torch.zeros((num_datapoints, dimension))\n",
    "ind_set = set()\n",
    "for f in tqdm(files):\n",
    "    data = torch.load(f)\n",
    "    ind = int(Path(f).name.split('|')[0])\n",
    "    Xs[ind] = data['mean_representations'][34]\n",
    "    ind_set.add(ind)\n",
    "print(len(ind_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "Principal Component Analysis is a popular technique for dimensionality reduction. Given `n_features` (1280 in our case), PCA computes a new set of `X` that \"best explain the data.\" We've found that this enables downstream models to be trained faster with minimal loss in performance.  \n",
    "\n",
    "Here, we set `X` to 60, but feel free to change it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(60)\n",
    "Xs_pca = pca.fit_transform(Xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='viz_representations'></a>\n",
    "## Visualize Representations\n",
    "\n",
    "Here, we plot the first two principal components on the x- and y- axes. Each point is then colored by its scaled effect (what we want to predict).\n",
    "\n",
    "Visually, we can see a separation based on color/effect, suggesting that our representations are useful for this task, without any task-specific training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dims = (7, 6)\n",
    "fig, ax = plt.subplots(figsize=fig_dims)\n",
    "sc = ax.scatter(Xs_pca[:,0], Xs_pca[:,1], c=ys, marker='.')\n",
    "ax.set_xlabel('PCA first principal component')\n",
    "ax.set_ylabel('PCA second principal component')\n",
    "plt.colorbar(sc, label='Variant Effect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='grid_search'></a>\n",
    "\n",
    "## Initialize / Run GridSearch\n",
    "\n",
    "We will run grid search for three different regression models:\n",
    "1. [K-nearest-neighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n",
    "2. [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html?highlight=svr#sklearn.svm.SVR)\n",
    "3. [Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html?highlight=randomforestregressor#sklearn.ensemble.RandomForestRegressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize grids for different regression techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_grid = {\n",
    "    'n_neighbors': [5, 10],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size' : [15, 30],\n",
    "    'p' : [1, 2],\n",
    "}\n",
    "\n",
    "svm_grid = {\n",
    "    'C' : [1.0],\n",
    "    'kernel' :['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'degree' : [3],\n",
    "    'gamma': ['scale'],\n",
    "}\n",
    "\n",
    "rfr_grid = {\n",
    "    'n_estimators' : [20],\n",
    "    'criterion' : ['mse', 'mae'],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'min_samples_split' : [5, 10],\n",
    "    'min_samples_leaf': [1, 4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_list = [KNeighborsRegressor, SVR, RandomForestRegressor]\n",
    "param_grid_list = [knn_grid, svm_grid, rfr_grid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Test Split\n",
    "\n",
    "Choose what fraction of the data to use for training. The Envision paper uses 80% of the data for training, but we find that pre-trained ESM representations require fewer downstream training examples to reach the same level of performance.\n",
    "\n",
    "Here, we will be using `Xs_pca`, because we observe it does just as well as `Xs` while allowing for faster training. You can easily swap it out for `Xs`.\n",
    "\n",
    "Some values to try out:\n",
    "- 0.01\n",
    "- 0.10\n",
    "- 0.30\n",
    "- 0.50\n",
    "- 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(Xs_pca, ys, train_size=train_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_train.shape, Xs_test.shape, len(ys_train), len(ys_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Grid Search \n",
    "\n",
    "(will take a few minutes on a single core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "grid_list = []\n",
    "for cls_name, param_grid in zip(cls_list, param_grid_list):\n",
    "    print(cls_name)\n",
    "    grid = GridSearchCV(\n",
    "        estimator = cls_name(), \n",
    "        param_grid = param_grid,\n",
    "        scoring = 'r2',\n",
    "        verbose = 1\n",
    "    )\n",
    "    grid.fit(Xs_train, ys_train)\n",
    "    result_list.append(pd.DataFrame.from_dict(grid.cv_results_))\n",
    "    grid_list.append(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='browse'></a>\n",
    "## Browse the Sweep Results\n",
    "\n",
    "The following tables show the top 5 parameter settings, based on `mean_test_score`. Given our setup, this should really be thought of as `validation_score`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list[0].sort_values('rank_test_score')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list[1].sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list[2].sort_values('rank_test_score')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eval'></a>\n",
    "## Evaluation\n",
    "\n",
    "Now that we have run grid search, each `grid` object contains a `best_estimator_`.\n",
    "\n",
    "We can use this to evaluate the correlation between our predictions and the true effect scores on the held-out validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grid in grid_list:\n",
    "    print(grid.best_estimator_)\n",
    "    print()\n",
    "    preds = grid.predict(Xs_test)\n",
    "    print(f'{scipy.stats.spearmanr(ys_test, preds)}')\n",
    "    print('\\n', '-' * 80, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM performs the best on the `test` set, with a spearman rho of 0.80! \n",
    "\n",
    "This is in line with our grid-search results, where it also had the best `validation` performance.\n",
    "\n",
    "In conclusion, our downstream model was able to use pre-trained ESM representations and obtain a decent result.\n",
    "\n",
    "(For reference, we report correlation of 0.89 in Table 7 of our paper, but this was achieved by fine-tuning the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-fairseq-20200421] *",
   "language": "python",
   "name": "conda-env-.conda-fairseq-20200421-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
